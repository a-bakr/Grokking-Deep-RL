{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bandits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "\n",
    "import gymnasium as gym\n",
    "# import gym\n",
    "import numpy as np\n",
    "import gym_bandits\n",
    "from scipy.special import softmax as softmax_fn\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "import sys\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "plt.style.use('fivethirtyeight')\n",
    "params = {\n",
    "    'figure.figsize': (15, 8),\n",
    "    'font.size': 24,\n",
    "    'legend.fontsize': 20,\n",
    "    'axes.titlesize': 28,\n",
    "    'axes.labelsize': 24,\n",
    "    'xtick.labelsize': 20,\n",
    "    'ytick.labelsize': 20\n",
    "}\n",
    "pylab.rcParams.update(params)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q = np.array([7, 3])\n",
    "np.argmax(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_exploitation(env, n_episodes=1000):\n",
    "    Q = np.zeros((env.action_space.n), dtype=np.float64)\n",
    "    N = np.zeros((env.action_space.n), dtype=np.int32)\n",
    "    \n",
    "    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n",
    "    returns = np.empty(n_episodes, dtype=np.float64)\n",
    "    actions = np.empty(n_episodes, dtype=np.int32)\n",
    "    \n",
    "    name = 'Pure exploitation'\n",
    "    es = tqdm(range(n_episodes), desc='Episodes for: ' + name, leave=False)\n",
    "    for e in es:\n",
    "        action = np.argmax(Q)\n",
    "        print(action)\n",
    "        print(\"env.step(action)\")\n",
    "        print(env.step(action))\n",
    "        \n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action]) / N[action]            \n",
    "        \n",
    "        Qe[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pure_exploration(env, n_episodes=1000):\n",
    "    Q = np.zeros((env.action_space.n), dtype=np.float64)\n",
    "    N = np.zeros((env.action_space.n), dtype=np.int32)\n",
    "    \n",
    "    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n",
    "    returns = np.empty(n_episodes, dtype=np.float64)\n",
    "    actions = np.empty(n_episodes, dtype=np.int32)\n",
    "    \n",
    "    name = 'Pure exploration'\n",
    "    es = tqdm(range(n_episodes), desc='Episodes for: ' + name, leave=False)\n",
    "    for e in es:\n",
    "        action = np.random.randint(len(Q))\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action]) / N[action]            \n",
    "        \n",
    "        Qe[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy(env, epsilon=0.1, n_episodes=1000):\n",
    "    Q = np.zeros((env.action_space.n), dtype=np.float64)\n",
    "    N = np.zeros((env.action_space.n), dtype=np.int32)\n",
    "    \n",
    "    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n",
    "    returns = np.empty(n_episodes, dtype=np.float64)\n",
    "    actions = np.empty(n_episodes, dtype=np.int32)\n",
    "    \n",
    "    name = 'Pure exploration'\n",
    "    es = tqdm(range(n_episodes), desc='Episodes for: ' + name, leave=False)\n",
    "    for e in es:\n",
    "        if np.random.random() <= epsilon:\n",
    "            action = np.random.randint(len(Q))\n",
    "        else:\n",
    "            action = np.argmax(len(Q))\n",
    "        \n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action]) / N[action]            \n",
    "        \n",
    "        Qe[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.uniform(low=2, high=100, size=(2, 2)), np.random.random((2, 2))\n",
    "max(0.2, .5)\n",
    "np.maximum(0.2, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.802\n",
      "0.6040000000000001\n",
      "0.4060000000000001\n",
      "0.20800000000000007\n",
      "0.010000000000000064\n",
      "0.01\n",
      "0.01\n",
      "0.01\n",
      "0.01\n"
     ]
    }
   ],
   "source": [
    "init_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_ratio = 0.5 \n",
    "n_episodes = 10\n",
    "\n",
    "def lin_decay(init_epsilon, min_epsilon, decay_ratio, n_episodes):\n",
    "    decay_range = init_epsilon - min_epsilon\n",
    "    decay_episodes = int(n_episodes * decay_ratio)\n",
    "    decay = decay_range / decay_episodes\n",
    "    return init_epsilon, decay\n",
    "\n",
    "epsilon, decay = lin_decay(init_epsilon, min_epsilon, decay_ratio, n_episodes)\n",
    "for e in range(n_episodes):\n",
    "    print(epsilon)\n",
    "    epsilon = max(epsilon - decay, min_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_dec_epsilon_greedy(\n",
    "    env, init_epsilon=1.0, min_epsilon=0.01, decay_ratio=0.05, n_episodes=1000):\n",
    "    Q = np.zeros((env.action_space.n), dtype=np.float64)\n",
    "    N = np.zeros((env.action_space.n), dtype=np.int32)\n",
    "    \n",
    "    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n",
    "    returns = np.empty(n_episodes, dtype=np.float64)\n",
    "    actions = np.empty(n_episodes, dtype=np.int32)\n",
    "    \n",
    "    name = f\"Lin Epsilon-Greedy {init_epsilon}, {min_epsilon}, {decay_ratio}\"\n",
    "    es = tqdm(\n",
    "        range(n_episodes), \n",
    "        desc='Episodes for: ' + name, \n",
    "        leave=False\n",
    "    )\n",
    "    \n",
    "    epsilon, decay = lin_decay(init_epsilon, min_epsilon, decay_ratio, n_episodes)\n",
    "    for e in es:\n",
    "        if np.random.random() <= epsilon:\n",
    "            action = np.random.randint(len(Q))\n",
    "        else:\n",
    "            action = np.argmax(len(Q))\n",
    "        epsilon = max(epsilon - decay, min_epsilon)\n",
    "        \n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action]) / N[action]            \n",
    "        \n",
    "        Qe[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010000\n",
      "0.010000\n",
      "0.010000\n",
      "0.010000\n",
      "0.010000\n",
      "0.010000\n",
      "0.010000\n",
      "0.010000\n",
      "0.010000\n",
      "0.010000\n"
     ]
    }
   ],
   "source": [
    "init_epsilon = 1.0\n",
    "min_epsilon = 0.01\n",
    "decay_ratio = 0.1\n",
    "n_episodes = 10\n",
    "\n",
    "def exp_decay(init_epsilon, min_epsilon, decay_ratio, n_episodes):\n",
    "    decay_episodes = int(n_episodes * decay_ratio)\n",
    "    min_epsilon = max(min_epsilon, 1e-10)\n",
    "    decay = np.exp(-np.log(init_epsilon / min_epsilon) / decay_episodes)\n",
    "    return init_epsilon, decay\n",
    "\n",
    "\n",
    "decay, epsilon = exp_decay(init_epsilon, min_epsilon, decay_ratio, n_episodes)\n",
    "for e in range(n_episodes):\n",
    "    print(f\"{epsilon:f}\")\n",
    "    epsilon = max(epsilon * decay, min_epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_dec_epsilon_greedy(\n",
    "    env, init_epsilon=1.0, min_epsilon=0.01, decay_ratio=0.05, n_episodes=1000):\n",
    "    Q = np.zeros((env.action_space.n), dtype=np.float64)\n",
    "    N = np.zeros((env.action_space.n), dtype=np.int32)\n",
    "    \n",
    "    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n",
    "    returns = np.empty(n_episodes, dtype=np.float64)\n",
    "    actions = np.empty(n_episodes, dtype=np.int32)\n",
    "    \n",
    "    name = f\"Exp Epsilon-Greedy {init_epsilon}, {min_epsilon}, {decay_ratio}\"\n",
    "    es = tqdm(\n",
    "        range(n_episodes), \n",
    "        desc='Episodes for: ' + name, \n",
    "        leave=False\n",
    "    )\n",
    "    \n",
    "    epsilon, decay = exp_decay(init_epsilon, min_epsilon, decay_ratio, n_episodes)\n",
    "    for e in es:\n",
    "        if np.random.random() <= epsilon:\n",
    "            action = np.random.randint(len(Q))\n",
    "        else:\n",
    "            action = np.argmax(len(Q))\n",
    "        epsilon = max(epsilon * decay, min_epsilon)\n",
    "        \n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action]) / N[action]            \n",
    "        \n",
    "        Qe[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100, 100])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.full((2), 1.0)\n",
    "np.full((2), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimistic_initialization(\n",
    "    env, optimistic_estimate=1.0, initial_count=100, n_episodes=1000):\n",
    "    Q = np.full((env.action_space.n), optimistic_estimate, dtype=np.float64)\n",
    "    N = np.full((env.action_space.n), initial_count, dtype=np.int32)\n",
    "    \n",
    "    Qe = np.empty((n_episodes, env.action_space.n), dtype=np.float64)\n",
    "    returns = np.empty(n_episodes, dtype=np.float64)\n",
    "    actions = np.empty(n_episodes, dtype=np.int32)\n",
    "    \n",
    "    name = f\"Optimistic {optimistic_estimate}, {initial_count}\"\n",
    "    es = tqdm(\n",
    "        range(n_episodes),\n",
    "        desc='Episodes for: ' + name, \n",
    "        leave=False\n",
    "    )\n",
    "    \n",
    "    for e in es:\n",
    "        action = np.argmax(Q)\n",
    "        next_state, reward, done, truncated, info = env.step(action)\n",
    "        N[action] += 1\n",
    "        Q[action] += (reward - Q[action]) / N[action]\n",
    "        \n",
    "        Qe[e] = Q\n",
    "        returns[e] = reward\n",
    "        actions[e] = action\n",
    "    return name, returns, Qe, actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-Armed Bandit environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import buffalo_gym\n",
    "\n",
    "SEEDS = (12, 34, 56, 78, 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OrderEnforcing' object has no attribute 'p_dist'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m env = gym.make(env_name, seed=seed)\n\u001b[32m      5\u001b[39m env.reset()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m b2_Q = np.array(\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mp_dist\u001b[49m * env.env.r_dist)\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mTwo-Armed Bandit environment with seed\u001b[39m\u001b[33m'\u001b[39m, seed)\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mProbability of reward:\u001b[39m\u001b[33m'\u001b[39m, env.env.p_dist)\n",
      "\u001b[31mAttributeError\u001b[39m: 'OrderEnforcing' object has no attribute 'p_dist'"
     ]
    }
   ],
   "source": [
    "b2_Vs = []\n",
    "for seed in SEEDS:\n",
    "    env_name = 'Bandit-v0'\n",
    "    env = gym.make(env_name, seed=seed)\n",
    "    env.reset()\n",
    "    \n",
    "    b2_Q = np.array(env.env.p_dist * env.env.r_dist)\n",
    "    \n",
    "    print('Two-Armed Bandit environment with seed', seed)\n",
    "    print('Probability of reward:', env.env.p_dist)\n",
    "    print('Reward:', env.env.r_dist)\n",
    "    print('Q(.):', b2_Q)\n",
    "    b2_Vs.append(np.max(b2_Q))\n",
    "    print('V*:', b2_Vs[-1])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def b2_run_simple_strategies_experiment(env_name='BanditTwoArmedUniform-v0'):\n",
    "    results = {}\n",
    "    experiments = [\n",
    "        # baseline strategies\n",
    "        lambda env: pure_exploitation(env),\n",
    "        lambda env: pure_exploration(env),\n",
    "        \n",
    "        # epsilon greedy\n",
    "        lambda env: epsilon_greedy(env, epsilon=0.07),\n",
    "        lambda env: epsilon_greedy(env, epsilon=0.1),\n",
    "        \n",
    "        # epsilon greedy linearly decaying\n",
    "        lambda env: lin_dec_epsilon_greedy(\n",
    "            env, init_epsilon=1.0, min_epsilon=0.0, decay_ratio=0.1\n",
    "        ),\n",
    "        lambda env: lin_dec_epsilon_greedy(\n",
    "            env, init_epsilon=0.3, min_epsilon=0.001, decay_ratio=0.1\n",
    "        ),\n",
    "        \n",
    "        # epsilon greedy exponentially decaying\n",
    "        lambda env: exp_dec_epsilon_greedy(\n",
    "            env, init_epsilon=1.0, min_epsilon=0.0, decay_ratio=0.1\n",
    "        ),\n",
    "        lambda env: exp_dec_epsilon_greedy(\n",
    "            env, init_epsilon=0.3, min_epsilon=0.0, decay_ratio=0.3\n",
    "        ),\n",
    "        \n",
    "        # epsilon greedy\n",
    "        lambda env: optimistic_initialization(\n",
    "            env, optimistic_estimate=1.0, initial_count=10\n",
    "        ),\n",
    "        lambda env: optimistic_initialization(\n",
    "            env, optimistic_estimate=1.0, initial_count=50\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    for env_seed in tqdm(SEEDS, desc='All experiments'):\n",
    "        env = gym.make(env_name, seed=env_seed)\n",
    "        env.reset()\n",
    "        true_Q = np.array(env.unwrapped.p_dist * env.unwrapped.r_dist)\n",
    "        opt_V = np.max(true_Q)\n",
    "        for seed in tqdm(SEEDS, desc='All environments', leave=False):\n",
    "            for experiment in tqdm(\n",
    "                experiments, desc=f'Experiments with seed {seed}', leave=False):\n",
    "                env.seed(seed); np.random.seed(seed); random.seed(seed)\n",
    "                name, Re, Qe, Ae = experiment(env)\n",
    "                Ae = np.expand_dims(Ae, -1)\n",
    "                \n",
    "                episode_mean_rew = np.cumsum(Re) / (np.arange(len(Re)) + 1)\n",
    "                Q_selected = np.take_along_axis(\n",
    "                    np.tile(true_Q, Ae.shape), Ae, axis=1).squeeze()\n",
    "                regret = opt_V - Q_selected\n",
    "                cum_regret = np.cumsum(regret)\n",
    "                \n",
    "                if name not in results.keys(): results[name] = {}\n",
    "                if 'Re' not in results[name].keys(): results[name]['Re'] = []\n",
    "                if 'Qe' not in results[name].keys(): results[name]['Qe'] = []\n",
    "                if 'Ae' not in results[name].keys(): results[name]['Ae'] = []\n",
    "                if 'cum_regret' not in results[name].keys(): \n",
    "                    results[name]['cum_regret'] = []\n",
    "                if 'episode_mean_rew' not in results[name].keys(): \n",
    "                    results[name]['episode_mean_rew'] = []\n",
    "\n",
    "                results[name]['Re'].append(Re)\n",
    "                results[name]['Qe'].append(Qe)\n",
    "                results[name]['Ae'].append(Ae)\n",
    "                results[name]['cum_regret'].append(cum_regret)\n",
    "                results[name]['episode_mean_rew'].append(episode_mean_rew)\n",
    "    return results\n",
    "\n",
    "b2_results_s = b2_run_simple_strategies_experiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running simple strategies on Two-Armed Bandit environments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results of simple strategies on Two-Armed Bandit environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running advanced strategies on Two-Armed Bandit environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results of advanced strategies on Two-Armed Bandit  environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10-Armed Gaussian Bandit environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running simple strategies on 10-Armed Bandit environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results of simple strategies on 10-Armed Bandit environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running advanced strategies on 10-Armed Bandit environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting results of advanced strategies on 10-Armed Bandit environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
