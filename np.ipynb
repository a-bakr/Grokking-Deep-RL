{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NumPy for Reinforcement Learning: A Comprehensive Tutorial\n",
    "===========================================================\n",
    "\n",
    "This tutorial covers how to effectively use NumPy for implementing reinforcement learning algorithms,\n",
    "from basic concepts to practical implementations.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple state representation: [0 0 1 0]\n",
      "Grid world representation:\n",
      " [[ 1.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0. 10.]]\n",
      "Frame stack shape: (4, 84, 84)\n",
      "Action probabilities: [0.1 0.2 0.5 0.2]\n",
      "Sum of probabilities: 1.0\n",
      "Randomly sampled action based on probabilities: 0\n"
     ]
    }
   ],
   "source": [
    "# ========================= PART 1: NUMPY BASICS FOR RL =========================\n",
    "\n",
    "\"\"\"\n",
    "1. NumPy Fundamentals for RL\n",
    "----------------------------\n",
    "NumPy provides efficient data structures and operations that are essential for RL:\n",
    "- Arrays for representing states, actions, rewards\n",
    "- Vectorized operations for faster computations\n",
    "- Random number generation for exploration strategies\n",
    "\"\"\"\n",
    "\n",
    "# Creating state representations\n",
    "# 1D arrays (for simple state/action spaces)\n",
    "simple_state = np.array([0, 0, 1, 0])  # One-hot encoding of a state\n",
    "print(\"Simple state representation:\", simple_state)\n",
    "\n",
    "# 2D arrays (for grid worlds or image-based environments)\n",
    "grid_world = np.zeros((4, 4))  # 4x4 grid world\n",
    "grid_world[0, 0] = 1  # Agent position\n",
    "grid_world[3, 3] = 10  # Goal position\n",
    "print(\"Grid world representation:\\n\", grid_world)\n",
    "\n",
    "# 3D arrays (for frame stacking in visual RL)\n",
    "frame_stack = np.zeros((4, 84, 84))  # 4 frames of 84x84 pixels\n",
    "print(\"Frame stack shape:\", frame_stack.shape)\n",
    "\n",
    "# Handling probability distributions (for policies)\n",
    "action_probs = np.array([0.1, 0.2, 0.5, 0.2])\n",
    "print(\"Action probabilities:\", action_probs)\n",
    "print(\"Sum of probabilities:\", np.sum(action_probs))\n",
    "\n",
    "# Random sampling (for exploration)\n",
    "random_action = np.random.choice(4, p=action_probs)  # Sample from discrete action space\n",
    "print(\"Randomly sampled action based on probabilities:\", random_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Q-table shape: (10, 4)\n",
      "Optimistic Q-table example:\n",
      " [[10. 10. 10. 10.]\n",
      " [10. 10. 10. 10.]\n",
      " [10. 10. 10. 10.]]\n",
      "Q-value for state 0, action 1: 1.5\n",
      "Best action for state 0: 1\n",
      "10 actions selected with epsilon-greedy: [np.int64(1), np.int64(1), np.int64(1), 0, np.int64(1), np.int64(1), np.int64(1), np.int64(1), 0, np.int64(1)]\n"
     ]
    }
   ],
   "source": [
    "# ========================= PART 2: IMPLEMENTING Q-TABLES =========================\n",
    "\n",
    "\"\"\"\n",
    "2. Q-Tables with NumPy\n",
    "----------------------\n",
    "Q-tables are fundamental for tabular reinforcement learning methods.\n",
    "NumPy enables efficient creation and manipulation of these tables.\n",
    "\"\"\"\n",
    "\n",
    "# Creating a Q-table for a simple environment\n",
    "# Rows = states, Columns = actions\n",
    "n_states, n_actions = 10, 4\n",
    "q_table = np.zeros((n_states, n_actions))\n",
    "print(\"Initial Q-table shape:\", q_table.shape)\n",
    "\n",
    "# Initializing with optimistic initial values to encourage exploration\n",
    "optimistic_q = np.ones((n_states, n_actions)) * 10.0\n",
    "print(\"Optimistic Q-table example:\\n\", optimistic_q[:3, :])\n",
    "\n",
    "# Accessing and updating Q-values\n",
    "state, action = 0, 1\n",
    "q_table[state, action] = 1.5\n",
    "print(f\"Q-value for state {state}, action {action}:\", q_table[state, action])\n",
    "\n",
    "# Finding the best action for a state (greedy policy)\n",
    "best_action = np.argmax(q_table[state])\n",
    "print(f\"Best action for state {state}:\", best_action)\n",
    "\n",
    "# Implementing epsilon-greedy action selection\n",
    "def epsilon_greedy(state, q_table, epsilon=0.1):\n",
    "    if np.random.random() < epsilon:\n",
    "        # Explore: select random action\n",
    "        return np.random.randint(q_table.shape[1])\n",
    "    else:\n",
    "        # Exploit: select best action\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# Example of multiple selections with epsilon-greedy\n",
    "actions = [epsilon_greedy(0, q_table) for _ in range(10)]\n",
    "print(\"10 actions selected with epsilon-greedy:\", actions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored state: [0.1 0.2 0.3 0.4]\n",
      "Stored action: 1\n",
      "Sampled batch shapes:\n",
      "- States: (4, 4)\n",
      "- Actions: (4,)\n",
      "- Rewards: (4,)\n"
     ]
    }
   ],
   "source": [
    "# ========================= PART 3: HANDLING TRANSITIONS =========================\n",
    "\n",
    "\"\"\"\n",
    "3. State Transitions and Rewards\n",
    "-------------------------------\n",
    "Efficiently managing and processing state transitions, rewards, and experiences\n",
    "is crucial for RL algorithms.\n",
    "\"\"\"\n",
    "\n",
    "# Simple experience replay buffer using NumPy\n",
    "buffer_size = 1000\n",
    "state_dim = 4\n",
    "\n",
    "# Initialize buffer components\n",
    "states = np.zeros((buffer_size, state_dim))\n",
    "actions = np.zeros(buffer_size, dtype=np.int32)\n",
    "rewards = np.zeros(buffer_size)\n",
    "next_states = np.zeros((buffer_size, state_dim))\n",
    "dones = np.zeros(buffer_size, dtype=np.bool_)\n",
    "\n",
    "# Function to store a transition\n",
    "def store_transition(buffer_idx, state, action, reward, next_state, done):\n",
    "    states[buffer_idx] = state\n",
    "    actions[buffer_idx] = action\n",
    "    rewards[buffer_idx] = reward\n",
    "    next_states[buffer_idx] = next_state\n",
    "    dones[buffer_idx] = done\n",
    "\n",
    "# Example of storing a transition\n",
    "store_transition(\n",
    "    0, \n",
    "    np.array([0.1, 0.2, 0.3, 0.4]),  # state\n",
    "    1,                               # action\n",
    "    0.5,                             # reward\n",
    "    np.array([0.2, 0.3, 0.4, 0.5]),  # next_state\n",
    "    False                            # done\n",
    ")\n",
    "\n",
    "print(\"Stored state:\", states[0])\n",
    "print(\"Stored action:\", actions[0])\n",
    "\n",
    "# Sampling batches for learning\n",
    "def sample_batch(batch_size=32):\n",
    "    indices = np.random.randint(0, buffer_size, size=batch_size)\n",
    "    return (\n",
    "        states[indices],\n",
    "        actions[indices],\n",
    "        rewards[indices],\n",
    "        next_states[indices],\n",
    "        dones[indices]\n",
    "    )\n",
    "\n",
    "# Example batch\n",
    "batch = sample_batch(4)\n",
    "print(\"Sampled batch shapes:\")\n",
    "print(\"- States:\", batch[0].shape)\n",
    "print(\"- Actions:\", batch[1].shape)\n",
    "print(\"- Rewards:\", batch[2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate value for state: 2.200\n",
      "Original weights: [ 1.   2.  -0.5  1.5]\n",
      "Updated weights: [ 0.999  1.995 -0.502  1.492]\n",
      "New prediction: 2.1906\n"
     ]
    }
   ],
   "source": [
    "# ========================= PART 4: VALUE FUNCTION APPROXIMATION =========================\n",
    "\n",
    "\"\"\"\n",
    "4. Value Function Approximation\n",
    "------------------------------\n",
    "Using NumPy for implementing linear value function approximation \n",
    "for states when the state space is too large for tabular methods.\n",
    "\"\"\"\n",
    "\n",
    "# Linear function approximation for state values\n",
    "def linear_value_function(state, weights):\n",
    "    return np.dot(state, weights)\n",
    "\n",
    "# Example state and weights\n",
    "state = np.array([0.1, 0.5, 0.2, 0.8])\n",
    "weights = np.array([1.0, 2.0, -0.5, 1.5])\n",
    "\n",
    "# Approximate value\n",
    "value = linear_value_function(state, weights)\n",
    "print(f\"Approximate value for state: {value:.3f}\")\n",
    "\n",
    "# Updating weights with gradient descent\n",
    "def update_weights(state, target, prediction, weights, alpha=0.01):\n",
    "    # TD error\n",
    "    error = target - prediction\n",
    "    # Gradient is just the feature vector (state) for linear approximation\n",
    "    gradient = state\n",
    "    # Update rule: w += α * δ * ∇_w V(s)\n",
    "    return weights + alpha * error * gradient\n",
    "\n",
    "# Example update\n",
    "target_value = 1.2\n",
    "prediction = linear_value_function(state, weights)\n",
    "new_weights = update_weights(state, target_value, prediction, weights)\n",
    "print(\"Original weights:\", weights)\n",
    "print(\"Updated weights:\", new_weights)\n",
    "print(\"New prediction:\", linear_value_function(state, new_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated Q-table for state 0:\n",
      "[0.05 0.   0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "# ========================= PART 5: PRACTICAL RL EXAMPLE =========================\n",
    "\n",
    "\"\"\"\n",
    "5. Implementing Q-Learning with NumPy\n",
    "-----------------------------------\n",
    "A complete example of tabular Q-learning for a simple environment.\n",
    "\"\"\"\n",
    "\n",
    "# Environment parameters (e.g., for a simple grid world)\n",
    "n_states = 16  # 4x4 grid\n",
    "n_actions = 4  # up, right, down, left\n",
    "\n",
    "# Learning parameters\n",
    "alpha = 0.1    # Learning rate\n",
    "gamma = 0.99   # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "\n",
    "# Initialize Q-table\n",
    "q_table = np.zeros((n_states, n_actions))\n",
    "\n",
    "# Epsilon-greedy policy\n",
    "def select_action(state, q_table, epsilon):\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.random.randint(n_actions)\n",
    "    else:\n",
    "        return np.argmax(q_table[state])\n",
    "\n",
    "# Q-learning update rule\n",
    "def update_q_value(state, action, reward, next_state, q_table, alpha, gamma):\n",
    "    # Q-learning formula: Q(s,a) = Q(s,a) + α[r + γ*max(Q(s',a')) - Q(s,a)]\n",
    "    best_next_action = np.argmax(q_table[next_state])\n",
    "    td_target = reward + gamma * q_table[next_state, best_next_action]\n",
    "    td_error = td_target - q_table[state, action]\n",
    "    q_table[state, action] += alpha * td_error\n",
    "    return q_table\n",
    "\n",
    "# Simulating one step of Q-learning (in a real implementation, you'd get these from the environment)\n",
    "state = 0\n",
    "action = select_action(state, q_table, epsilon)\n",
    "next_state = 1  # Assume this is the result of taking action from state\n",
    "reward = 0.5    # Assume this is the reward received\n",
    "\n",
    "# Update Q-table\n",
    "q_table = update_q_value(state, action, reward, next_state, q_table, alpha, gamma)\n",
    "\n",
    "print(\"Updated Q-table for state 0:\")\n",
    "print(q_table[0])\n",
    "\n",
    "# In a complete implementation, you would repeat this process for many episodes\n",
    "# to converge to an optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed values for 100 states at once, first 5: [1.28464004 0.68591776 0.70948735 0.66844749 0.64442645]\n",
      "Advantages shape: (100, 3)\n",
      "Example advantages for one state: [0.19617703 0.11518982 0.10468136]\n",
      "Softmax output (action probabilities):\n",
      "[[0.16718888 0.06509881 0.76771231]\n",
      " [0.21657053 0.69355549 0.08987399]\n",
      " [0.09829053 0.8013178  0.10039167]\n",
      " [0.61301286 0.13247829 0.25450885]\n",
      " [0.55102166 0.28627143 0.16270692]]\n",
      "Sum of probabilities for each state: [1. 1. 1. 1. 1.]\n",
      "One-hot encoded actions:\n",
      " [[0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# ========================= PART 6: ADVANCED TECHNIQUES =========================\n",
    "\n",
    "\"\"\"\n",
    "6. Advanced NumPy Techniques for RL\n",
    "---------------------------------\n",
    "More sophisticated operations useful for implementing RL algorithms.\n",
    "\"\"\"\n",
    "\n",
    "# Vectorized operations for batch processing\n",
    "states = np.random.rand(100, 4)  # 100 states of dimension 4\n",
    "weights = np.random.rand(4)\n",
    "\n",
    "# Vectorized value computation for all states at once\n",
    "values = np.dot(states, weights)\n",
    "print(\"Computed values for 100 states at once, first 5:\", values[:5])\n",
    "\n",
    "# Computing advantages (A = Q - V) for advantage actor-critic\n",
    "q_values = np.random.rand(100, 3)  # Q-values for 100 states, 3 actions\n",
    "v_values = np.random.rand(100)     # V-values for 100 states\n",
    "\n",
    "# Broadcasting to compute advantages\n",
    "advantages = q_values - v_values[:, np.newaxis]\n",
    "print(\"Advantages shape:\", advantages.shape)\n",
    "print(\"Example advantages for one state:\", advantages[0])\n",
    "\n",
    "# Softmax for policy output\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "logits = np.random.randn(5, 3)  # Logits for 5 states, 3 actions\n",
    "action_probs = softmax(logits)\n",
    "print(\"Softmax output (action probabilities):\")\n",
    "print(action_probs)\n",
    "print(\"Sum of probabilities for each state:\", np.sum(action_probs, axis=1))\n",
    "\n",
    "# One-hot encoding for discrete actions\n",
    "actions = np.array([1, 0, 2, 1, 0])\n",
    "one_hot_actions = np.eye(3)[actions]\n",
    "print(\"One-hot encoded actions:\\n\", one_hot_actions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
